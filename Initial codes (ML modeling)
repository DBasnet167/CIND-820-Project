# --- 7. Model Training and Prediction ---
print("\n--- Step 7: Model Training and Prediction ---")

# Evaluation Function
def evaluate_model(y_true, y_pred, name):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)
    print(f'\n{name} Forecast Performance:')
    print(f'MAE: {mae:.6f}')
    print(f'RMSE: {rmse:.6f}')
    print(f'RÂ²: {r2:.4f}')

# Naive Forecast
# CRITICAL FIX HERE: Naive forecast should use the UNSEALED lag_1 from the original df
# Get the 'lag_1' column from the original df before any scaling or subsetting for X_train/X_test
# Ensure that the 'lag_1' from df aligns with the y_test index.
# The `y` series itself should align perfectly with `df['log_diff']`
# The `X` dataframe contains the features, so `y_test` is already aligned with `X_test`
# We need the *actual* previous values for the test period.
# These previous values are precisely what the 'lag_1' column of the *original, unscaled* df represents
# for the entries corresponding to the X_test index.

try:
    # Filter the original df to get the unscaled 'lag_1' values corresponding to the test set dates
    # X_test.index contains the dates for which we need naive forecasts
    naive_pred = df.loc[X_test.index, 'lag_1'].values # <--- THIS IS THE FIX
    evaluate_model(y_test, naive_pred, 'Naive Forecast')
except KeyError:
    print("\nError: 'lag_1' column not found in original df or index mismatch for Naive Forecast.")
    naive_pred = np.full_like(y_test, np.nan) # Fill with NaNs to allow plotting


# Random Forest Regressor
print("\nTraining Random Forest Regressor...")
rf_model = RandomForestRegressor(n_estimators=500, max_leaf_nodes=50, max_features='sqrt', random_state=42)
rf_model.fit(X_train, y_train)
rf_pred = rf_model.predict(X_test)
evaluate_model(y_test, rf_pred, 'Random Forest Forecast')

# Gradient Boosting Regressor
print("\nTraining Gradient Boosting Regressor...")
gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
gb_model.fit(X_train, y_train)
gb_pred = gb_model.predict(X_test)
evaluate_model(y_test, gb_pred, 'Gradient Boosting Forecast')

# K-Nearest Neighbors Regressor
print("\nTraining K-Nearest Neighbors Regressor...")
knn_model = KNeighborsRegressor(n_neighbors=5)
knn_model.fit(X_train, y_train)
knn_pred = knn_model.predict(X_test)
evaluate_model(y_test, knn_pred, 'kNN Forecast')


# --- 8. Plot All Predictions ---
print("\n--- Step 8: Plotting All Predictions ---")
plt.figure(figsize=(14, 7))
plt.plot(y_test.index, y_test.values, label='Actual (log_diff)', color='black', linewidth=2)
plt.plot(y_test.index, naive_pred, '--', label='Naive Forecast', color='purple', alpha=0.7)
plt.plot(y_test.index, rf_pred, ':', label='Random Forest Forecast', color='red', alpha=0.7)
plt.plot(y_test.index, gb_pred, '-.', label='Gradient Boosting Forecast', color='green', alpha=0.7)
plt.plot(y_test.index, knn_pred, '--', label='kNN Forecast', color='blue', alpha=0.7)

plt.title('CPI Inflation Forecast Comparison (Log-Differenced CPI)')
plt.ylabel('Inflation (log diff)')
plt.xlabel('Date')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# --- 9. Feature Importance for Tree-Based Models ---
print("\n--- Step 9: Feature Importance ---")

print("\n--- Random Forest Feature Importance ---")
if hasattr(rf_model, 'feature_importances_'):
    feature_importances_rf = pd.Series(rf_model.feature_importances_, index=X.columns).sort_values(ascending=False)
    print(feature_importances_rf)
else:
    print("Random Forest model does not have feature_importances_ attribute.")

print("\n--- Gradient Boosting Feature Importance ---")
if hasattr(gb_model, 'feature_importances_'):
    feature_importances_gb = pd.Series(gb_model.feature_importances_, index=X.columns).sort_values(ascending=False)
    print(feature_importances_gb)
else:
    print("Gradient Boosting model does not have feature_importances_ attribute.")

print("\n--- Script Finished ---")
