##### 7. Model Training and Prediction
- The function evaluate_model is designed to assess the performance of a regression model by calculating and displaying three common evaluation metrics. It takes as input the true target values: y_true, the predicted values from the mode: y_pred, and a string: name, to label the output.
- Inside the function it first computes the Mean Absolute Error, which measures the average magnitude of errors between predictions and actual values, without considering their direction. It then calculates the Root Mean Squared Error and then the R-squared score which indicates the proportion of variance in the target variable that is explained by the model; value closer to 1 means better predictive performance. 

#Evaluation Function
def evaluate_model(y_true, y_pred, name):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)
    print(f'\n{name} Forecast Performance:')
    print(f'MAE: {mae:.6f}')
    print(f'RMSE: {rmse:.6f}')
    print(f'R²: {r2:.4f}')

## Naive Forecast (ŷₜ = yₜ₋₁)
try: 
    naive_pred = df.loc[X_test.index, 'lag_1'].values
    evaluate_model(y_test, naive_pred, 'Naive')
except KeyError:
    print("\nError: 'lag_1' column not found in original df or index mismatch for Naive Forecast.")
    naive_pred = np.full_like(y_test, np.nan) # Fill with NaNs to allow plotting

# Random Forest Regressor
rf_model = RandomForestRegressor(n_estimators=500, max_leaf_nodes=50, max_features='sqrt', random_state=42)
rf_model.fit(X_train, y_train)
rf_pred = rf_model.predict(X_test)
evaluate_model(y_test, rf_pred, 'Random Forest')

#Gradient Boosting Regressor:
gb_model = GradientBoostingRegressor(n_estimators=500, learning_rate=0.1, max_depth=3, random_state=42)
gb_model.fit(X_train, y_train)
gb_pred = gb_model.predict(X_test)
evaluate_model(y_test, gb_pred, 'Gradient Boosting Forecast')

# K-Nearest Neighbors Regressor
knn_model = KNeighborsRegressor(n_neighbors=5)
knn_model.fit(X_train, y_train)
knn_pred = knn_model.predict(X_test)
evaluate_model(y_test, knn_pred, 'kNN Forecast')

##### 8. Plot all predictions

#Plot all predictions.
plt.figure(figsize=(14, 7))
plt.plot(y_test.index, y_test.values, label='Actual (log_diff)', color='black', linewidth=2)
plt.plot(y_test.index, naive_pred, '--', label='Naive Forecast', color='purple', alpha=0.7)
plt.plot(y_test.index, rf_pred, ':', label='Random Forest Forecast', color='red', alpha=0.7)
plt.plot(y_test.index, gb_pred, '-.', label='Gradient Boosting Forecast', color='green', alpha=0.7)
plt.plot(y_test.index, knn_pred, '--', label='kNN Forecast', color='blue', alpha=0.7)

plt.title('CPI Inflation Forecast Comparison (Log-Differenced CPI)')
plt.ylabel('Inflation (log diff)')
plt.xlabel('Date')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()


##### 9. Feature Importance for Tree-Based models

- Feature importance values indicate how much each input feature contributed to the model's predictions, helping to understand which variables are most influential. 

print("\n--- Random Forest Feature Importance ---")
if hasattr(rf_model, 'feature_importances_'):
    feature_importances_rf = pd.Series(rf_model.feature_importances_, index=X.columns).sort_values(ascending=False)
    print(feature_importances_rf)
else:
    print("Random Forest model does not have feature_importances_ attribute.")

print("\n--- Gradient Boosting Feature Importance ---")
if hasattr(gb_model, 'feature_importances_'):
    feature_importances_gb = pd.Series(gb_model.feature_importances_, index=X.columns).sort_values(ascending=False)
    print(feature_importances_gb)
else:
    print("Gradient Boosting model does not have feature_importances_ attribute.")

# Add this code to create a more detailed results comparison
results_df = pd.DataFrame({
    'Model': ['Naive Forecast', 'Random Forest', 'Gradient Boosting', 'kNN'],
    'MAE': [0.003367, 0.002337, 0.002553, 0.002459],
    'RMSE': [0.003883, 0.002651, 0.002920, 0.002712],
    'R²': [-0.4116, 0.3420, 0.2016, 0.3115],
    'Improvement over Naive (MAE)': ['Baseline', '30.6%', '24.2%', '27.0%']
})
print(results_df.to_string(index=False))
